{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Guardrails with Chakoshi × Weave Scorers — A Practical Cookbook\n",
        "\n",
        "This notebook shows how to implement the `chakoshi` moderation API as a Weave `Scorer` and apply it as a guardrail for LLM outputs. It focuses on a minimal, production-minded setup with clear patterns you can reuse.\n",
        "\n",
        "- Goal: Evaluate outputs with a `Scorer` and block/replace responses when unsafe\n",
        "- Audience: Developers adopting Weave `Scorer` + `call.apply_scorer` in production\n",
        "- Prereqs: `WANDB_API_KEY`, `CHAKOSHI_API_KEY` (optionally `CHAKOSHI_CATEGORY_SET_ID`). For the OpenAI example, `OPENAI_API_KEY`.\n",
        "\n",
        "The following cells keep the flow simple and to the point. If you need an account, sign up at the [Chakoshi Platform](https://platform.beta.chakoshi.ntt.com/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install weave requests --quiet\n",
        "# If you want to use the optional OpenAI example, uncomment the next line:\n",
        "# %pip install openai --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import weave\n",
        "\n",
        "PROJECT_NAME = \"chakoshi-guardrails-cookbook\"\n",
        "weave.init(PROJECT_NAME)\n",
        "\n",
        "CHAKOSHI_API_KEY = os.getenv(\"CHAKOSHI_API_KEY\")\n",
        "CHAKOSHI_CATEGORY_SET_ID = os.getenv(\"CHAKOSHI_CATEGORY_SET_ID\")\n",
        "\n",
        "if not CHAKOSHI_API_KEY:\n",
        "    print(\"[WARN] CHAKOSHI_API_KEY is not set. Please set it before running.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configure a custom detection Category Set in the Chakoshi UI\n",
        "\n",
        "This cookbook assumes you have created and enabled a custom “Category Set” in the Chakoshi UI.\n",
        "\n",
        "- Example rule: “Time travel/time slip topics are prohibited.” Save this as part of a Category Set.\n",
        "- Click “Copy selected Category Set ID” in the UI to get the ID.\n",
        "- Set it as an environment variable, for example:\n",
        "\n",
        "```bash\n",
        "export CHAKOSHI_CATEGORY_SET_ID=\"copied-category-set-id\"\n",
        "```\n",
        "\n",
        "The Scorer will pass this ID to the API so the UI-defined rules are enforced. That’s why in our examples, topics like DeLorean, Doraemon, and Asahina (time travel) are flagged—fun, right?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "class ChakoshiModerationScorer(weave.Scorer):\n",
        "    \"\"\"\n",
        "    A Weave Scorer that calls Chakoshi `/v1/judge/text` to evaluate output safety.\n",
        "\n",
        "    Args:\n",
        "        model (str): Chakoshi model ID to use for moderation.\n",
        "        category_set_id (str | None): Category Set ID; if omitted, the API default is used.\n",
        "        timeout_seconds (float): HTTP timeout seconds.\n",
        "\n",
        "    Returns:\n",
        "        dict: Score result fields:\n",
        "            - flagged (bool): Whether the content is unsafe.\n",
        "            - reason (str | None): Short reason when unsafe.\n",
        "            - unsafe_score (float | None): Unsafe score (if provided).\n",
        "            - unsafe_category (str | None): Category detected.\n",
        "            - label_str (str | None): Label string.\n",
        "\n",
        "    Examples:\n",
        "        >>> scorer = ChakoshiModerationScorer()\n",
        "        >>> result = scorer.score(\"This is a safe text\")\n",
        "        >>> isinstance(result[\"flagged\"], bool)\n",
        "        True\n",
        "    \"\"\"\n",
        "\n",
        "    model: str = \"chakoshi-moderation-241223\"\n",
        "    category_set_id: Optional[str] = None\n",
        "    timeout_seconds: float = 10.0\n",
        "\n",
        "    @weave.op\n",
        "    def score(self, output: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate the given output text using the Chakoshi API.\n",
        "\n",
        "        Args:\n",
        "            output (str): The text to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            dict: The score result (flagged, reason, etc.).\n",
        "        \"\"\"\n",
        "        if not CHAKOSHI_API_KEY:\n",
        "            return {\"flagged\": False, \"reason\": \"CHAKOSHI_API_KEY is not set\", \"unsafe_score\": None, \"unsafe_category\": None, \"label_str\": None}\n",
        "\n",
        "        url = \"https://api.beta.chakoshi.ntt.com/v1/judge/text\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {CHAKOSHI_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"accept\": \"application/json\",\n",
        "        }\n",
        "        payload: Dict[str, Any] = {\n",
        "            \"input\": str(output),\n",
        "            \"model\": self.model,\n",
        "        }\n",
        "        if self.category_set_id or CHAKOSHI_CATEGORY_SET_ID:\n",
        "            payload[\"category_set_id\"] = self.category_set_id or CHAKOSHI_CATEGORY_SET_ID\n",
        "\n",
        "        try:\n",
        "            resp = requests.post(url, headers=headers, json=payload, timeout=self.timeout_seconds)\n",
        "        except Exception as e:\n",
        "            return {\"flagged\": False, \"reason\": f\"Chakoshi API error: {e}\", \"unsafe_score\": None, \"unsafe_category\": None, \"label_str\": None}\n",
        "\n",
        "        if resp.status_code != 200:\n",
        "            return {\"flagged\": False, \"reason\": f\"Chakoshi API non-200: {resp.status_code}\", \"unsafe_score\": None, \"unsafe_category\": None, \"label_str\": None}\n",
        "\n",
        "        data: Dict[str, Any] = resp.json() if resp.content else {}\n",
        "        results: Dict[str, Any] = data.get(\"results\", {}) if isinstance(data, dict) else {}\n",
        "        unsafe_flag = bool(results.get(\"unsafe_flag\", False))\n",
        "        label_str = results.get(\"label_str\")\n",
        "        unsafe_score = results.get(\"unsafe_score\")\n",
        "        unsafe_category = results.get(\"unsafe_category\")\n",
        "\n",
        "        return {\n",
        "            \"flagged\": unsafe_flag,\n",
        "            \"reason\": (f\"Chakoshi flagged: {unsafe_category}, score: {unsafe_score} ({label_str})\" if unsafe_flag else None),\n",
        "            \"unsafe_score\": unsafe_score,\n",
        "            \"unsafe_category\": unsafe_category,\n",
        "            \"label_str\": label_str,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@weave.op\n",
        "def generate_response(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Simple demo responder (LLM stand-in) used for illustrating guardrails.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): User input prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: A demo response string.\n",
        "\n",
        "    Examples:\n",
        "        >>> generate_response(\"hello\")[0]  # doctest: +ELLIPSIS\n",
        "        'hello response...'\n",
        "    \"\"\"\n",
        "    return f\"{prompt} response...\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def process_with_guardrail(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Apply Chakoshi guardrails using Weave's `call.apply_scorer` and decide whether\n",
        "    to block or allow the response.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): User input prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The final response after guardrails.\n",
        "\n",
        "    Examples:\n",
        "        >>> import asyncio\n",
        "        >>> asyncio.run(process_with_guardrail(\"safe topic\"))  # doctest: +ELLIPSIS\n",
        "        'safe topic response...'\n",
        "    \"\"\"\n",
        "    response, call = generate_response.call(prompt)\n",
        "\n",
        "    # Apply the Scorer\n",
        "    evaluation = await call.apply_scorer(\n",
        "        ChakoshiModerationScorer(\n",
        "            name=\"chakoshi-guardrail\",\n",
        "            category_set_id=CHAKOSHI_CATEGORY_SET_ID,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if evaluation.result.get(\"flagged\"):\n",
        "        return f\"[BLOCKED] {evaluation.result.get('reason') or 'Unsafe content detected'}\"\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Minimal example ===\")\n",
        "res = await process_with_guardrail(\"Tell me about the future of AI\")\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Optional: Realistic setup with OpenAI\n",
        "\n",
        "- Call OpenAI with an async `weave.op`\n",
        "- Apply `ChakoshiModerationScorer` to the output\n",
        "- Replace when unsafe\n",
        "\n",
        "If you don’t use OpenAI, you can skip this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# from openai import AsyncOpenAI  # 使う場合のみ\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "@weave.op()\n",
        "async def generate_text_with_openai(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Call OpenAI asynchronously to generate a response (optional example).\n",
        "    In production, tune the model and parameters accordingly.\n",
        "    \"\"\"\n",
        "    if not OPENAI_API_KEY:\n",
        "        # Fallback for demo if no API key\n",
        "        return f\"[no-openai] {prompt} response...\"\n",
        "\n",
        "    # Example if you want to actually call OpenAI (commented out):\n",
        "    # client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
        "    # resp = await client.chat.completions.create(\n",
        "    #     model=\"gpt-4o\",\n",
        "    #     messages=[\n",
        "    #         {\"role\": \"system\", \"content\": \"Respond politely in English or Japanese.\"},\n",
        "    #         {\"role\": \"user\", \"content\": prompt},\n",
        "    #     ],\n",
        "    # )\n",
        "    # return resp.choices[0].message.content\n",
        "\n",
        "    return f\"[simulated-openai] {prompt} response...\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def guarded_generation_with_openai(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Apply Chakoshi guardrails to OpenAI output.\n",
        "    The UI Category Set ID (CHAKOSHI_CATEGORY_SET_ID) is passed for evaluation.\n",
        "    \"\"\"\n",
        "    result, call = await generate_text_with_openai.call(prompt)\n",
        "\n",
        "    chk = await call.apply_scorer(\n",
        "        ChakoshiModerationScorer(\n",
        "            name=\"chakoshi-guardrail\",\n",
        "            category_set_id=CHAKOSHI_CATEGORY_SET_ID,\n",
        "        )\n",
        "    )\n",
        "    if chk.result.get(\"flagged\"):\n",
        "        return f\"[BLOCKED] {chk.result.get('reason') or 'Unsafe content detected'}\"\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== OpenAI example (optional) ===\")\n",
        "resp = await guarded_generation_with_openai(\"How much energy does it take to go back to the future with a DeLorean?\")\n",
        "print(resp)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Ops tips\n",
        "\n",
        "- Split Weave projects by use case: `weave.init(\"project-name\")`\n",
        "- You can apply multiple `Scorer`s (toxicity, hallucination, PII, etc.)\n",
        "- To optimize cost, apply `call.apply_scorer` to a subset of traffic\n",
        "- Use tiered guardrails: strict for critical requests, lighter checks for routine ones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Batch test ===\")\n",
        "\n",
        "test_prompts = [\n",
        "    \"Recommend travel spots\",\n",
        "    \"How much energy to time travel with a DeLorean?\",\n",
        "    \"Tell me about AI ethics\",\n",
        "]\n",
        "\n",
        "for p in test_prompts:\n",
        "    out = await process_with_guardrail(p)\n",
        "    print(f\"Input: {p}\\nOutput: {out}\\n\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Why do time-travel topics get blocked?\n",
        "\n",
        "In these tests, we use a Chakoshi Category Set configured in the UI with a custom rule like “time travel/time slip topics are prohibited.” As a result, references such as DeLorean, Doraemon, and Asahina (time travel) are flagged by the Scorer and blocked by the guardrail.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
